{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression\n",
    "---\n",
    "\n",
    "\n",
    "## Nathan Karst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Overview</h1>\n",
    "\n",
    "<ul>\n",
    "<li>Linear regression is a bread-and-butter technique from classical statistics, and so it makes a natural starting point for investigating a fundamental question: \n",
    "</li>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<font size=\"6\"><center>How is a big data mindset different <br> than a traditional statistical mindset?</center></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Review\n",
    "\n",
    "* In linear regression, we have a variable $Y$ that we're trying to predict using the information from a collection of variables $X_1, X_2, \\ldots, X_p$. \n",
    "\n",
    "\n",
    "* In traditional statistics, we often call $Y$ the *dependent variable* and the $X_i$ the *indepdent variables*.\n",
    "\n",
    "\n",
    "* In big data, we often call $Y$ the *target* and the $X_i$ the *predictors* or *inputs*.\n",
    "\n",
    "\n",
    "* We assume that a very particular relationship holds: \n",
    "<br>\n",
    "<br>\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon.$$\n",
    "<br>\n",
    "* All of the information in the model is contained in the $\\beta$s, called the *coefficients* of the linear model. \n",
    "\n",
    "\n",
    "* For every increase in $X_i$ by 1 unit, $Y$ on average increases by $\\beta_i$. \n",
    "\n",
    "\n",
    "* The variable $\\epsilon$ contains all the information in $Y$ that is not captured by the linear trends. We hope that this is small -- sometimes it is, and sometimes it isn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review\n",
    "\n",
    "* If we make a prediction $\\hat{Y}_i$ for a given observation $Y_i$, then the error is just the difference between the two: \n",
    "<br>\n",
    "<br>\n",
    "$$\\mbox{error}_i = Y_i - \\hat{Y}_i.$$\n",
    "<br>\n",
    "* Under this definition, error is positive if we *underestimate* and negative if we *overestimate*.\n",
    "\n",
    "\n",
    "* Because errors can be positive and negative, we can't just sum them to get a measure of total error over all predictions -- positive and negative errors could cancel out to 0, even if the model were not perfect! \n",
    "\n",
    "\n",
    "* We instead look at the **sum of squared errors** (SSE): \n",
    "<br>\n",
    "<br>\n",
    "$$ \\mbox{SSE} = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2.$$\n",
    "<br>\n",
    "* Our linear regression model produces estimates $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ that minimize SSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Old and new\n",
    "\n",
    "* But all of this is just what we learned in traditional statistics? So how is big data any different? \n",
    "\n",
    "\n",
    "* In classical statistics, \n",
    "    * We have very limited data.\n",
    "    * We want to create a rule to *explain* how $Y$ depends on the $X_i$. \n",
    "    * We assess model performance based on how well our model fits *old* data.\n",
    "    \n",
    "    \n",
    "* In big data,\n",
    "    * We typically have *lots* of data.\n",
    "    * We want to create a rule to *predict* how $Y$ will be have.\n",
    "    * We assess model performance based on how accurately it predicts *new* data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The data analytics tool flow\n",
    "\n",
    "![alt text](https://www.dropbox.com/s/wlc6ffqb8k96h7b/6_training_test_flow.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import BabsonAnalytics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load and manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "   LSTAT  MEDV  \n",
       "0   4.98  24.0  \n",
       "1   9.14  21.6  \n",
       "2   4.03  34.7  \n",
       "3   2.94  33.4  \n",
       "4   5.33  36.2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/BostonHousing.csv')\n",
    "df = df.drop('CAT. MEDV',axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We need to split the data set into two pieces: a training set that we will use to build the model, and a test set we'll use to evaluate model performance.\n",
    "* Typical splits range anywhere from 60-40 to 80-20, with the larger piece going to training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train = df.sample(frac=0.8) # sample 80% of the rows from df\n",
    "test = df.drop(train.index) # drop all that are now in training\n",
    "\n",
    "trainTarget = train.pop('MEDV') # pop out the target from df...\n",
    "testTarget = test.pop('MEDV') # ...we'll need these separately later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build\n",
    "\n",
    "* Now that we have the training and test data set up, we're ready to build the model.\n",
    "* This phase looks different for each model type (e.g., linear regression).\n",
    "* The parts are always the same:\n",
    "    * Create an empty model.\n",
    "    * Fit the model to the data.\n",
    "    * (Inspect the model if you want to...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Estimate  p-value\n",
      "Predictor                     \n",
      "(Intercept)     40.79    0.000\n",
      "CRIM            -0.13    0.000\n",
      "ZN               0.05    0.001\n",
      "INDUS           -0.05    0.491\n",
      "CHAS             2.68    0.005\n",
      "NOX            -15.77    0.000\n",
      "RM               3.76    0.000\n",
      "AGE             -0.01    0.471\n",
      "DIS             -1.67    0.000\n",
      "RAD              0.25    0.001\n",
      "TAX             -0.01    0.005\n",
      "PTRATIO         -0.92    0.000\n",
      "LSTAT           -0.50    0.000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model \n",
    "model = linear_model.LinearRegression(fit_intercept=True) # build an empty linear model\n",
    "model.fit(train,trainTarget) # fit the model to the training data\n",
    "BabsonAnalytics.inspectLinearModel(train,trainTarget,model) # inspect the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predict\n",
    "\n",
    "* With the model in hand, we can now make some predictions. \n",
    "* Remember, we're always predicting on the test data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluate\n",
    "\n",
    "* We now have to figure out whether our predictions are any good.\n",
    "\n",
    "\n",
    "* The definition of \"good\" is different for numerical and categorical variables.\n",
    "\n",
    "\n",
    "* For numeric variables, we have two measures of quality, MAPE and RMSE:\n",
    "<br>\n",
    "<br>\n",
    "$$\\mbox{MAPE} = \\sum_{i=1}^N \\frac{|Y_i - \\hat{Y}_i|}{Y_i}, \\quad \\quad \\mbox{RMSE} = \\sqrt{\\sum_{i=1}^N (Y_i - \\hat{Y}_i)^2}$$\n",
    "<br>\n",
    "* MAPE is measured in terms of percent deviation, and RMSE is measured in whatever the units of the target are, e.g., here thousands of dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  5.17132565941\n",
      "MAPE:  0.20199536658108916\n"
     ]
    }
   ],
   "source": [
    "error = testTarget - predictions\n",
    "rmse = np.sqrt(np.mean(error**2))\n",
    "mape = np.mean(np.abs(error/testTarget))\n",
    "print('RMSE: ', rmse)\n",
    "print('MAPE: ', mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your turn\n",
    "\n",
    "* Create a new notebook.\n",
    "* Load the data located in ToyotaCorolla.csv.\n",
    "* Change `Automatic` and `Met_Color` to categorical variables (if they are not already).\n",
    "* Create a linear model for `Price`, and fit this model to the training data.\n",
    "* Using this model, make predictions for the observations in the test partition. \n",
    "* Compute the MAPE and RMSE associated with your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variable Selection\n",
    "\n",
    "* Given a collection of predctions, how can we find the *best* model we could construct? \n",
    "\n",
    "\n",
    "* Imagine we have a data set with 50 columns. \n",
    "\n",
    "\n",
    "* There are $2^{50} \\approx 1 \\times 10^{15}$ linear models we could make with these variables. \n",
    "\n",
    "\n",
    "* This is a large-ish number. If we could evaluate one model per second, it would take us 31 million years to get through them all.\n",
    "\n",
    "\n",
    "* If we have 60 variables (just 10 more!), it would take 36 billion years.\n",
    "\n",
    "\n",
    "* Obviously, checking all possible models is not feasible. We need to be more clever.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variable Selection\n",
    "\n",
    "* Here we'll use *recursive feature elimination* (RFE) to prune back the number of variables we're using. \n",
    "\n",
    "\n",
    "* We first look at all variables, find the one that contributes least to the predictive power of the model, and remove it.\n",
    "\n",
    "* We then look at the model created by all the remaining variables, find the one that contributes the least, and remove it. \n",
    "\n",
    "\n",
    "* We continue doing this until we have a model with $k$ variables, where $k$ is a number we have specified beforehand. \n",
    "\n",
    "\n",
    "* The hope here is that we'll end up with a smaller collection of variables that do about as good of a job at predicting the target as the full data set. \n",
    "\n",
    "\n",
    "* A smaller number of predictors are cheaper and easier to collect, clean, and store, and fewer data points means faster fitting and predicting. \n",
    "\n",
    "\n",
    "* Moreover, we can clearly draw a line between the variables that matter and the variables that don't. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Estimate  p-value\n",
      "Predictor                     \n",
      "(Intercept)     35.74      0.0\n",
      "CHAS             3.96      0.0\n",
      "NOX            -13.66      0.0\n",
      "RM               3.89      0.0\n",
      "DIS             -1.07      0.0\n",
      "PTRATIO         -0.99      0.0\n",
      "LSTAT           -0.63      0.0\n",
      "\n",
      "\n",
      "\n",
      "RMSE:  5.3876432516\n",
      "MAPE:  0.21591567871015066\n"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_selection\n",
    "model = linear_model.LinearRegression();\n",
    "rfe = feature_selection.RFE(model,6)\n",
    "rfe.fit(train,trainTarget)\n",
    "\n",
    "inspectLinearModel(train=train,trainTarget=trainTarget,model=rfe)\n",
    "\n",
    "predictions = rfe.predict(test)\n",
    "\n",
    "error = testTarget - predictions\n",
    "rmse = np.sqrt(np.mean(error**2))\n",
    "mape = np.mean(np.abs(error/testTarget))\n",
    "print('RMSE: ', rmse)\n",
    "print('MAPE: ', mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Notice that now all variables are significant, and that the MAPE and RMSE aren't much different from those of the full model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
